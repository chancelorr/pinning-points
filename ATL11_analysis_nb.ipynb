{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "By Chance\n",
    "\n",
    "Loads and processes ATL11 data\n",
    "\n",
    "Input: .ini config file which fills all variables in preamble\n",
    "\n",
    "Output: a bunch of figures\n",
    "\tMap of dh/dt\n",
    "\tMap of elevation\n",
    "\tEnsemble Median Anomaly\n",
    "\tEnsemble Relative Elevation Count\n",
    "\tEnsemble dh/dt distribution\n",
    "\tEnsemble Median Elevations\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# System libraries\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import configparser\n",
    "\n",
    "#Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rasterio as rs\n",
    "\n",
    "#For geometries\n",
    "from shapely import box, LineString, Point, Polygon\n",
    "from shapely.geometry.polygon import orient\n",
    "\n",
    "#For REMA\n",
    "from rasterio import plot\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "#Datetime\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import time\n",
    "\n",
    "#For plotting, ticking, and line collection\n",
    "from matplotlib import cm \n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import cmcrameri.cm as cmc\n",
    "import earthpy.spatial as es\n",
    "\n",
    "#Personal and application specific utilities\n",
    "from utils.nsidc import download_is2\n",
    "#from utils.S2 import plotS2cloudfree, add_inset, convert_time_to_string\n",
    "from utils.utilities import is2dt2str\n",
    "\n",
    "#For error handling\n",
    "import traceback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basin: Cp-D\n",
      "region: all\n",
      "output_dir: /Volumes/nox2/Chance/data/is2/ATL11_Cp-D_all\n",
      "uid: ccroberts\n",
      "pwd: **********\n",
      "email: ccroberts@ucsd.edu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ini = 'config/Cp-D_all_lumos.ini'\n",
    "\n",
    "######## Load variables ###########\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read(ini)\n",
    "\n",
    "#os and pyproj paths\n",
    "gdal_data = config.get('os', 'gdal_data')\n",
    "proj_lib = config.get('os', 'proj_lib')\n",
    "proj_data = config.get('os', 'proj_data')\n",
    "\n",
    "#path params\n",
    "basin = config.get('data', 'basin')\n",
    "region = config.get('data', 'region')\n",
    "shape = f'shapes/{basin}_{region}.shp'\n",
    "output_dir = config.get('data', 'output_dir')\n",
    "rema_path = config.get('data', 'rema_path')\n",
    "try: fig_dir = config.get('data', 'fig_dir')\n",
    "except: fig_dir='figs'\n",
    "\n",
    "#access params\n",
    "uid = config.get('access', 'uid')\n",
    "pwd = config.get('access', 'pwd')\n",
    "email = config.get('access', 'email')\n",
    "\n",
    "#Print results\n",
    "os.environ[\"GDAL_DATA\"] = gdal_data # need to specify to make gdal work\n",
    "os.environ[\"PROJ_LIB\"] = proj_lib # need to specify to make pyproj work\n",
    "os.environ[\"PROJ_DATA\"] = proj_data # need to specify to make pyproj work\n",
    "\n",
    "print(f\"basin: {basin}\")\n",
    "print(f\"region: {region}\")\n",
    "print(f\"output_dir: {output_dir}\")\n",
    "\n",
    "print(f\"uid: {uid}\")\n",
    "print(f\"pwd: {'*'*len(pwd)}\")\n",
    "print(f\"email: {email}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Code ####################\n",
    "crs_antarctica = 'EPSG:3031'\n",
    "crs_latlon = 'EPSG:4326'\n",
    "short_name = 'ATL11'\n",
    "# Read shapefile into gdf for everything\n",
    "gdf = gpd.read_file(shape).set_crs(crs_latlon, allow_override=True).to_crs(crs_antarctica)\n",
    "\n",
    "# Separate by entry type\n",
    "gdf_fl = gdf[gdf.Id_text=='Ice shelf']\n",
    "gdf_pp = gdf[(gdf.Id_text=='Ice rise or connected island')]\n",
    "gdf_ext = gpd.GeoDataFrame(geometry=[gdf_fl.apply(lambda p: Polygon(p.geometry.exterior.coords), axis=1).unary_union.union(gdf_pp.unary_union)],\n",
    "    crs=crs_antarctica).explode(ignore_index=True)\n",
    "gdf_gr = gdf[gdf.Id==1]\n",
    "gdf_ext_all = gpd.GeoDataFrame(geometry=[gdf_ext.unary_union.union(gdf_gr.unary_union)], crs=crs_antarctica)\n",
    "\n",
    "# gdf for subsetting\n",
    "#gdf = gdf[~(gdf.Id==1)]\n",
    "gdf = gpd.GeoDataFrame(geometry=[gdf.unary_union], crs=crs_antarctica)\n",
    "\n",
    "### Define a bunch of functions (folded)\n",
    "def download_data():\n",
    "    uid,pw,eml = getedcreds()\n",
    "    download_is2(short_name='ATL11', uid=uid, pwd=pw, email=eml, output_dir=output_dir, shape=shape, shape_subset=shape)\n",
    "    print('saved files to %s' % output_dir)\n",
    "    return output_dir\n",
    "\n",
    "def get_file_info():\n",
    "    search_for = '%s_' % short_name\n",
    "    search_in = output_dir + '/'\n",
    "    filelist = [search_in+f for f in os.listdir(search_in) \\\n",
    "                if os.path.isfile(os.path.join(search_in, f)) & (search_for in f) & ('.h5' in f)]\n",
    "    filelist.sort()\n",
    "    print('There are %i files.' % len(filelist))\n",
    "    \n",
    "    dirdict = dict([(x,'ascending') for x in [1,2,3,12,13,14]] + \\\n",
    "                   [(x,'descending') for x in [5,6,7,8,9,10]] + \\\n",
    "                   [(x,'turning') for x in [4,11]])\n",
    "    df_files = pd.DataFrame({'filename': filelist})\n",
    "    df_files['granule_id'] = df_files.apply(lambda x: x.filename[x.filename.rfind(search_for):], axis=1)\n",
    "    df_files['tides_filename'] = df_files.apply(lambda x: f'{output_dir}/tides/ATL11_CATS2008-v2023_TIDES_{x.granule_id[6:]}', axis=1)\n",
    "    df_files['track'] = df_files.apply(lambda x: int(x.granule_id[6:10]), axis=1)\n",
    "    df_files['region'] = df_files.apply(lambda x: int(x.granule_id[10:12]), axis=1)\n",
    "    df_files['direction'] = df_files.apply(lambda x: dirdict[x.region], axis=1)\n",
    "    df_files['cycles'] = df_files.apply(lambda x: '%s-%s' % (x.granule_id[13:15],x.granule_id[15:17]), axis=1)\n",
    "    df_files['version'] = df_files.apply(lambda x: int(x.granule_id[18:21]), axis=1)\n",
    "    df_files['release'] = df_files.apply(lambda x: int(x.granule_id[22:24]), axis=1)\n",
    "    return df_files\n",
    "\n",
    "def getedcreds():\n",
    "    # change your credentials here, do not push them to github! \n",
    "    uid = uid\n",
    "    pwd = pwd\n",
    "    email = email\n",
    "\n",
    "    # to print a message if they haven't been changed\n",
    "    if uid == '<your_nasa_earthdata_user_id>':\n",
    "        print('\\n WARNING: YOU NEED TO SET UP YOUR NASA EARTHDATA CREDENTIALS TO DOWNLOAD ICESAT-2 DATA!\\n')\n",
    "        print('  update the info in ed/edcreds.py :\\n')\n",
    "        print(\"  def getedcreds():\")\n",
    "        print(\"    # change your credentials here, do not push them to github!\")\n",
    "        print(\"    uid = '<your_nasa_earthdata_user_id>'\")\n",
    "        print(\"    pwd = '<your_nasa_earthdata_password>'\")\n",
    "        print(\"    email = '<your_nasa_earthdata_account_email>'\")\n",
    "        return None\n",
    "    else:\n",
    "        return uid, pwd, email\n",
    "\n",
    "def is2dt2str(lake_mean_delta_time):\n",
    "    lake_mean_delta_time = np.mean(lake_mean_delta_time)\n",
    "    if np.isnan(lake_mean_delta_time) | (lake_mean_delta_time == np.inf):\n",
    "        return np.nan\n",
    "    else:\n",
    "        ATLAS_SDP_epoch_datetime = datetime(2018, 1, 1, tzinfo=timezone.utc)\n",
    "        ATLAS_SDP_epoch_timestamp = datetime.timestamp(ATLAS_SDP_epoch_datetime)\n",
    "        lake_mean_timestamp = ATLAS_SDP_epoch_timestamp + lake_mean_delta_time\n",
    "        lake_mean_datetime = datetime.fromtimestamp(lake_mean_timestamp, tz=timezone.utc)\n",
    "        time_format_out = '%Y-%m-%dT%H:%M:%SZ'\n",
    "        is2time = datetime.strftime(lake_mean_datetime, time_format_out)\n",
    "        return is2time\n",
    "\n",
    "def set_axis_color(ax, axcolor):\n",
    "    ax.spines['bottom'].set_color(axcolor)\n",
    "    ax.spines['top'].set_color(axcolor) \n",
    "    ax.spines['right'].set_color(axcolor)\n",
    "    ax.spines['left'].set_color(axcolor)\n",
    "    ax.tick_params(axis='x', colors=axcolor)\n",
    "    ax.tick_params(axis='y', colors=axcolor)\n",
    "    ax.yaxis.label.set_color(axcolor)\n",
    "    ax.xaxis.label.set_color(axcolor)\n",
    "    ax.title.set_color(axcolor)\n",
    "\n",
    "def get_ground_tracks(datadict):\n",
    "    crs_latlon = 'EPSG:4326'\n",
    "    gts = []\n",
    "    for k in datadict.keys():\n",
    "        ds = datadict[k]\n",
    "        gdf_gt = gpd.GeoDataFrame(geometry=gpd.points_from_xy(ds.longitude, ds.latitude), crs=crs_latlon)\n",
    "        #for 3d geometry\n",
    "        #gdf_gt = gpd.GeoDataFrame(geometry=gpd.points_from_xy(ds.longitude, ds.latitude, ds.h_ano.sel(track=ds.track.data[0], pt=k).mean(dim='cycle_number')), crs=crs_latlon)\n",
    "        gdf_gt['pt'] = k\n",
    "        gts.append(gdf_gt)\n",
    "    gdf_gts = gpd.GeoDataFrame(geometry=pd.concat(gts).groupby(['pt'])[['geometry']].apply(lambda x: LineString(x.geometry.tolist()))\n",
    "        ).reset_index().set_crs(crs_latlon)\n",
    "    colordict = {'col0': 'darkblue', 'col1': 'rebeccapurple', 'col2': 'palevioletred', 'col3': 'thistle'}\n",
    "    gdf_gts['figcolor'] = gdf_gts.apply(lambda x: colordict['col%s' % (int(x.pt[2])-1)], axis=1)\n",
    "    gdf_gts['track'] = ds.track.data[0]\n",
    "    \n",
    "    return gdf_gts\n",
    "\n",
    "def read_atl11(filename, track, verbose=False):\n",
    "    if verbose: print(f'reading track: {track}')\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        datadict = {}\n",
    "        pts = [x for x in f.keys() if 'pt' in x]\n",
    "        for pt in pts:\n",
    "            try:\n",
    "                vars_data = ['delta_time', 'h_corr', 'h_corr_sigma', 'h_corr_sigma_systematic', 'quality_summary']\n",
    "                vars_coords = ['cycle_number', 'latitude', 'longitude','ref_pt']\n",
    "                ds = xr.Dataset({**{v: (['x', 'cycle_number', 'track', 'pt'], f[pt][v][()][:, :, np.newaxis, np.newaxis]) for v in vars_data},\n",
    "                    'geoid': (['x', 'track', 'pt'], f[pt]['ref_surf/geoid_h'][()][:, np.newaxis, np.newaxis])},\n",
    "                    coords={'cycle_number': f[pt]['cycle_number'][()],\n",
    "                    **{v : ('x', f[pt][v][()]) for v in vars_coords[1:]}})\n",
    "                ds.coords['x'], ds['track'], ds['pt'] = np.arange(len(ds.x)), [track], [pt]\n",
    "                ds = ds.assign_coords(x_atc=('x', np.arange(len(f[pt]['latitude'][()])) * 60))\n",
    "                h_arr = np.array(ds.h_corr-ds.geoid)# go to numpy for 2-d boolean indexing\n",
    "                h_arr[ds.quality_summary>0] = np.nan\n",
    "                h_arr[(h_arr>3e2)+(h_arr<-50)] = np.nan\n",
    "                ds['h_corr'] = (ds.h_corr.dims, h_arr)\n",
    "                ds['h_corr'] = ds.h_corr+ds.geoid\n",
    "                datadict[pt] = ds\n",
    "            #except KeyError as e:\n",
    "            #    print(f\"KeyError: The key {e} was not found in the data source.\")\n",
    "            #except ValueError as e:\n",
    "            #    print(f\"ValueError: {e}\")\n",
    "            #except Exception as e:\n",
    "            #    print(f\"An unexpected error occurred: {e}\")\n",
    "            except: continue\n",
    "    return datadict\n",
    "\n",
    "def read_atl11_tides(filename, track):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        tidedict = {}\n",
    "        pts = [x for x in f.keys() if 'pt' in x]\n",
    "        for pt in pts:\n",
    "            vars_data = ['delta_time', 'cycle_stats/tide_ocean']\n",
    "            vars_coords = ['cycle_number', 'latitude', 'longitude', 'ref_pt']\n",
    "            ds = xr.Dataset({v: (['x', 'cycle_number', 'track', 'pt'], f[pt][v][()][:, :, np.newaxis, np.newaxis]) for v in vars_data}, \n",
    "                    coords={'cycle_number': f[pt]['cycle_number'][()],\n",
    "                    **{v : ('x', f[pt][v][()]) for v in vars_coords[1:]}})\n",
    "            ds.coords['x'], ds['track'], ds['pt'] = np.arange(len(ds.x)), [track], [pt]\n",
    "            ds = ds.assign_coords(x_atc=('x', np.arange(len(f[pt]['latitude'][()])) * 60))\n",
    "            ds = ds.rename({'cycle_stats/tide_ocean': 'tide_cats'})\n",
    "            tide_cats = np.array(ds.tide_cats) # go to numpy for 2-d boolean indexing\n",
    "            tide_cats[tide_cats>1e5]=np.nan\n",
    "            ds['tide_cats'] = (ds.tide_cats.dims, tide_cats)\n",
    "            #ds['tide_cats'] = ds.tide_cats.interpolate_na(dim='x', method='linear').interpolate_na(dim='x', method='nearest', fill_value='extrapolate')\n",
    "            tidedict[pt] = ds\n",
    "    return tidedict\n",
    "\n",
    "def get_data(track, verbose=False):\n",
    "    # get the data\n",
    "    filename = df_files[df_files.track == track].filename.iloc[0]\n",
    "    tides_filename = df_files[df_files.track == track].tides_filename.iloc[0]\n",
    "    datadict = read_atl11(filename, track, verbose)\n",
    "    tidedict = read_atl11_tides(tides_filename, track)\n",
    "    datadict = {pt: xr.merge([datadict[pt], tidedict[pt]], join='inner', compat='override') for pt in datadict}\n",
    "    datadict = {pt: datadict[pt].assign(h_abs=(('x', 'cycle_number', 'track', 'pt'), \n",
    "        (datadict[pt]['h_corr']-datadict[pt]['geoid']-datadict[pt]['tide_cats']).data)) for pt in datadict}\n",
    "    datadict = {pt: datadict[pt].assign(h_ano=(('x', 'cycle_number', 'track', 'pt'),\n",
    "        (datadict[pt].h_abs-datadict[pt].h_abs.median(dim='cycle_number')).data)) for pt in datadict}\n",
    "    gdf_gts = get_ground_tracks(datadict).to_crs(crs_antarctica)\n",
    "    return datadict, gdf_gts\n",
    "\n",
    "'''\n",
    "def clip_data(datadict, gdf_gts, mask):\n",
    "    #clip through rectangle is faster\n",
    "    datadict_clipped = {}\n",
    "    gdf_gts_clipped_list = []\n",
    "    for pt in datadict:\n",
    "        ds=datadict[pt]\n",
    "        #select pt\n",
    "        gdf_this = gdf_gts[gdf_gts.pt==pt]\n",
    "        #convert linestring to points\n",
    "        gdf_this_pts = gpd.GeoDataFrame(geometry=gdf_gts[gdf_gts.pt==pt].get_coordinates(ignore_index=True).apply(lambda l: Point(l), axis=1), \n",
    "            crs=crs_antarctica)\n",
    "        gdf_clipped = gdf_this_pts.clip(mask.clip(gdf_this.bounds.values[0]))\n",
    "        try: gdf_gts_clipped_list.append(gdf_this.clip(mask))\n",
    "        except: continue\n",
    "        datadict_clipped[pt] = ds.sel(x=(gdf_clipped.index))\n",
    "    return datadict_clipped, pd.concat(gdf_gts_clipped_list, ignore_index=True)\n",
    "'''\n",
    "\n",
    "def clip_data(datadict, gdf_gts, mask):\n",
    "    #clip through rectangle is faster\n",
    "    datadict_clipped = {}\n",
    "    gdf_gts_clipped_list = []\n",
    "    for pt in datadict:\n",
    "        ds=datadict[pt]\n",
    "        #select pt\n",
    "        gt_this = gdf_gts[gdf_gts.pt==pt]\n",
    "        #convert linestring to points\n",
    "        gt_this_pts = gpd.GeoDataFrame(geometry=gt_this.get_coordinates(ignore_index=True).apply(lambda l: Point(l), axis=1), \n",
    "            crs=crs_antarctica)\n",
    "        gdf_clipped_index = gt_this_pts.clip(mask.clip(gt_this.bounds.values[0])).index\n",
    "        try: gdf_gts_clipped_list.append(gt_this.clip(mask))\n",
    "        except: continue\n",
    "        datadict_clipped[pt] = ds.sel(x=(gdf_clipped_index))\n",
    "    return datadict_clipped, pd.concat(gdf_gts_clipped_list, ignore_index=True)\n",
    "\n",
    "def get_ds_dict(tracklist, mask=None):\n",
    "    ds_list, gdf_gts_clipped_list = [], []\n",
    "    for t in tracklist: \n",
    "        datadict, gdf_gts_clipped = get_data(t)\n",
    "        if mask is not None: datadict, gdf_gts_clipped = clip_data(datadict, gdf_gts_clipped, mask)\n",
    "        ds_add = xr.concat([datadict[pt] for pt in datadict], dim='pt')\n",
    "        ds_add['x'] = np.arange(len(ds_add.x))\n",
    "        ds_list.append(ds_add.sortby('x'))\n",
    "        gdf_gts_clipped_list.append(gdf_gts_clipped)\n",
    "    print('generating dict and gdf')\n",
    "    return {ds.track.values[0]: ds for ds in ds_list}, pd.concat(gdf_gts_clipped_list, ignore_index=True)\n",
    "\n",
    "def combine_ds_dict(ds_dict):\n",
    "    ds_list = []\n",
    "    for t in ds_dict:\n",
    "        ds = ds_dict[t]\n",
    "        if len(ds.x)!=0: \n",
    "            ds['x'] = np.arange(len(ds.x))\n",
    "            ds_list.append(ds)\n",
    "    return xr.concat(ds_list, dim='track')\n",
    "\n",
    "def get_stats(tracklist, mask=None):\n",
    "    ds_list = []\n",
    "    gdf_gts_list = []\n",
    "    for t in tracklist:\n",
    "        datadict, gdf_gts_clipped = get_data(t)\n",
    "        # subset here\n",
    "        if mask is not None: datadict, gdf_gts_clipped = clip_data(datadict, gdf_gts_clipped, mask)\n",
    "        gdf_gts_list.append(gdf_gts_clipped)\n",
    "        for pt in datadict:\n",
    "            try:\n",
    "                ds = datadict[pt]\n",
    "                h_abs = ds.h_corr - ds.geoid - ds.tide_cats\n",
    "                coords_dict = {'cycle_number': ds.cycle_number.data,\n",
    "                 'track': ds.track.data,\n",
    "                 'pt': ds.pt.data}\n",
    "                stat_dict = {'h_min': h_abs.min(dim='x'), \n",
    "                 'h_max': h_abs.max(dim='x'), \n",
    "                 'h_sum': h_abs.sum(dim='x'),\n",
    "                 'h_mean': h_abs.mean(dim='x'), \n",
    "                 'h_med': h_abs.median(dim='x'), \n",
    "                 'h_ano': ds.h_ano.median(dim='x'),\n",
    "                 'h_std': h_abs.std(dim='x', skipna=True, ddof=1),\n",
    "                 'h_var': h_abs.var(dim='x', skipna=True, ddof=1), \n",
    "                 't_count': h_abs.count(dim='x'), \n",
    "                 'pct_nan': h_abs.count(dim='x')/h_abs.sizes['x'], \n",
    "                 't_dist': h_abs.count(dim='x')*0+h_abs.x_atc.max()/1000,\n",
    "                 'tide_min': ds.tide_cats.min(dim='x'),\n",
    "                 'tide_max': ds.tide_cats.max(dim='x'),\n",
    "                 'tide_mean': ds.tide_cats.mean(dim='x'),\n",
    "                 'tide_sum': ds.tide_cats.sum(dim='x')}\n",
    "                dss = xr.Dataset({v: (['cycle_number', 'track', 'pt'], stat_dict[v].data) for v in stat_dict}, coords={v: coords_dict[v] for v in coords_dict})\n",
    "                ds_list.append(dss)\n",
    "            except: \n",
    "                continue\n",
    "            #except KeyError as e:\n",
    "            #    print(f'failed for track {t}, {pt}')\n",
    "            #    print(f\"KeyError: The key {e} was not found in the data source.\")\n",
    "            #except ValueError as e:\n",
    "            #    print(f'failed for track {t}, {pt}')\n",
    "            #    print(f\"ValueError: {e}\")\n",
    "            #except Exception as e:\n",
    "            #    print(f'failed for track {t}, {pt}')\n",
    "            #    print(f\"An unexpected error occurred: {e}\")\n",
    "    return xr.combine_by_coords(data_objects=ds_list), pd.concat(gdf_gts_list, ignore_index=True)\n",
    "        \n",
    "def reproject_raster(src, target_crs):\n",
    "    '''\n",
    "    Change crs of imported data\n",
    "    '''\n",
    "    \n",
    "    src_crs = src.crs\n",
    "    src_transform = src.transform\n",
    "    src_width = src.width\n",
    "    src_height = src.height\n",
    "\n",
    "    # Define the target CRS\n",
    "    target_crs = target_crs\n",
    "\n",
    "    # Reproject the raster data to the target CRS\n",
    "    reprojected_data, dst_transform = rs.warp.reproject(\n",
    "        source=rs.band(src, 1),\n",
    "        src_transform=src_transform,\n",
    "        src_crs=src_crs,\n",
    "        dst_crs=target_crs,\n",
    "        resampling=rs.enums.Resampling.nearest)\n",
    "    \n",
    "    return reprojected_data, dst_transform, target_crs\n",
    "\n",
    "def retry(num_attempts=1, sleep_time=5):\n",
    "    for i in range(num_attempts): \n",
    "        try: \n",
    "            download_data()\n",
    "            print(f'success on attempt {i}')\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if i==num_attempts-1:\n",
    "                print(f\"The following error occurred: {e}\")\n",
    "                traceback.print_exc()\n",
    "            time.sleep(sleep_time)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322 files.\n",
      "Reading netCDFs into XR datasets...DONE\n",
      "Reading ESRI shapefiles into geodataframes...DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####\n",
    "\n",
    "## load data\n",
    "\n",
    "# Get_ds_dict, pretty fast\n",
    "df_files=get_file_info()\n",
    "#print('Loading data')\n",
    "tracklist=df_files.track\n",
    "#print('Generating data dicts')\n",
    "#ds_dict, gdf_gts_all = get_ds_dict(tracklist, gdf_ext)\n",
    "#ds_dict_fl, gdf_gts_fl = get_ds_dict(tracklist, gdf_fl)\n",
    "#ds_dict_pp, gdf_gts_pp = get_ds_dict(tracklist, gdf_pp)\n",
    "\n",
    "# Combine for full dataset\n",
    "#print('Combining into xarray datasets')\n",
    "#ds_all = combine_ds_dict(ds_dict)\n",
    "#ds_fl = combine_ds_dict(ds_dict_fl)\n",
    "#ds_pp = combine_ds_dict(ds_dict_pp)\n",
    "\n",
    "processed_dir='/Volumes/nox2/Chance/processed_data/'\n",
    "print('Reading netCDFs into XR datasets...', end='', flush=True)\n",
    "ds_all = xr.open_dataset(f'{processed_dir}/{basin}_ds_all.nc')\n",
    "ds_gr = xr.open_dataset(f'{processed_dir}/{basin}_ds_gr.nc')\n",
    "ds_fl = xr.open_dataset(f'{processed_dir}/{basin}_ds_fl.nc')\n",
    "ds_pp = xr.open_dataset(f'{processed_dir}/{basin}_ds_pp.nc')\n",
    "print('DONE')\n",
    "\n",
    "print('Reading ESRI shapefiles into geodataframes...', end='', flush=True)\n",
    "gdf_gts_all = gpd.read_file(f'{processed_dir}/{basin}_gdf_gts_all.shp', engine='pyogrio')\n",
    "gdf_gts_gr = gpd.read_file(f'{processed_dir}/{basin}_gdf_gts_gr.shp', engine='pyogrio')\n",
    "gdf_gts_fl = gpd.read_file(f'{processed_dir}/{basin}_gdf_gts_fl.shp', engine='pyogrio')\n",
    "gdf_gts_pp = gpd.read_file(f'{processed_dir}/{basin}_gdf_gts_pp.shp', engine='pyogrio')\n",
    "print('DONE')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot functions\n",
    "\n",
    "## make plot function\n",
    "def make_map(h_plot=None, dpi=600, rolling=None, vlims=[-5, 5], plot_rema=False, save=False, transparent=True):\n",
    "    \n",
    "    # figure setup\n",
    "    major_font_size = 12\n",
    "    minor_font_size = 10\n",
    "    line_w = 0.5\n",
    "    \n",
    "    # make figure and axes\n",
    "    fig, ax = plt.subplots(figsize=[13,8], dpi=dpi)\n",
    "    \n",
    "    # plot the basemap and ground tracks\n",
    "\n",
    "    if plot_rema:\n",
    "        with rs.open(rema_path) as src:\n",
    "            dem = src.read()\n",
    "            tr = src.transform\n",
    "            bbox = box(*gdf_ext_all.total_bounds)\n",
    "            src_masked, src_masked_tr = mask(src, shapes=[gdf_ext_all.geometry[0]], \n",
    "                crop=True, filled=False)\n",
    "            src_hs = np.ma.masked_array(es.hillshade(src_masked[0].filled(np.nan)), mask=src_masked.mask[0])\n",
    "            plot.show(src_hs, ax=ax, transform=src_masked_tr, cmap='Greys_r', \n",
    "                aspect='equal', vmin=-150, vmax=100)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_axis_off()\n",
    "    \n",
    "    #add ground tracks\n",
    "    lc_list=[]\n",
    "    if h_plot is not None:\n",
    "        for i in range(len(gdf_gts_all)):\n",
    "            row = gdf_gts_all[i:i+1]\n",
    "            coords = row.get_coordinates()\n",
    "            cmap = cmc.vik_r\n",
    "            t = row.track.iloc[0]\n",
    "            pt = row.pt.iloc[0]\n",
    "            # Prepare segments for LineCollection\n",
    "            # Prepare segments for LineCollection\n",
    "            dc_dx = np.gradient(coords.x)\n",
    "            dc_dx_idx = np.abs(dc_dx)>1e2\n",
    "            coords.loc[dc_dx_idx, 'x'] = np.nan\n",
    "            points = np.array([coords.x, coords.y]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            # Create a LineCollection\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=plt.Normalize(vlims[0], vlims[1]))\n",
    "            if rolling is not None: \n",
    "                try: \n",
    "                    h_plot = np.array(pd.Series(h_plot.sel(track=t, pt=pt)).rolling(window=rolling, center=True, min_periods=1, \n",
    "                    win_type='gaussian').mean(std=rolling/3))\n",
    "                    print('succesfful')\n",
    "                except: continue\n",
    "            else: h_plot = h_plot.sel(track=t, pt=pt)\n",
    "            lc.set_array(h_plot)\n",
    "            lc.set_linewidth(line_w)\n",
    "            lc_list.append(lc)\n",
    "            ax.add_collection(lc)\n",
    "    elif h_plot is None:\n",
    "        gdf_gts_all.plot(ax=ax, color=gdf_gts_all.plotcolor, linewidth=line_w)\n",
    "        gdf_gts_pp.plot(ax=ax, color='lightblue', linewidth=line_w)\n",
    "    \n",
    "    hdls = []\n",
    "    gdf_ext_all.apply(lambda p: p.buffer(4e3)).plot(ax=ax, color='None', edgecolor='white', label='grounded ice', linewidth=1, zorder=500)\n",
    "    gdf_pp.plot(ax=ax, color='None', edgecolor='black', label='grounded ice', linewidth=0.2, zorder=501)\n",
    "    gdf_ext.apply(lambda p: p.buffer(1e3)).plot(ax=ax, color='None', edgecolor='royalblue', label='floating ice', linewidth=0.4, zorder=502)\n",
    "    \n",
    "\n",
    "    cfig, cax = plt.subplots(figsize=[4, 0.25])\n",
    "    #cax.axis('off')\n",
    "    #ax.text(0.5, 0.96, f'ICESat-2 ATL11 Height Change\\n Basin {basin} 2019-2024', transform=ax.transAxes, \n",
    "    #        ha='center', va='center', fontsize=major_font_size, bbox=boxprops, zorder=502)\n",
    "    # Create a ScalarMappable with the same colormap and normalization\n",
    "    norm = mcolors.Normalize(vmin=vlims[0], vmax=vlims[1])\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  # Only needed for older versions of matplotlib\n",
    "    \n",
    "    # Add the inset axis\n",
    "    #pos = ax.get_position()\n",
    "    #cax_pos = [pos.x0+pos.width*0.7, pos.y0+0.13, pos.width*0.25, pos.height*0.01]\n",
    "    #cax_pos = [0, 0, 4, 0.5]\n",
    "    #cax = fig.add_axes(cax_pos)\n",
    "    #fig.patches.append(Rectangle((cax_pos[0]-, cax_pos[1]), cax_pos[2]*1.25, cax_pos[3]*2,\n",
    "    #    transform=fig.transFigure, color='white', zorder=1))\n",
    "    \n",
    "    # Add a colorbar to the fig, with a specific location\n",
    "    cbar = cfig.colorbar(sm, cax=cax, orientation='horizontal', fraction=0.1, pad=0.0)\n",
    "    cbar.ax.tick_params(labelsize=minor_font_size) \n",
    "    cbar.set_label('ICESat-2 height change \\n 2018-2024 (m yr$^{-1}$)', fontsize=minor_font_size)\n",
    "    cbar.outline.set_edgecolor('black')\n",
    "    cbar.outline.set_linewidth(0.5)\n",
    "    set_axis_color(cax, 'black')\n",
    "    \n",
    "    # Customize the colorbar ticks if needed\n",
    "    cbar.set_ticks([vlims[0], 0, vlims[1]])\n",
    "    #cbar.set_ticklabels(['0', '0.2', '0.4', '0.6', '0.8', '1'])\n",
    "    \n",
    "    figname = f'{fig_dir}/{basin}_dhdt_map.png'\n",
    "    print(figname)\n",
    "    cbarname = f'{fig_dir}/{basin}_dhdt_map_cbar.png'\n",
    "    if rolling is not None: figname = f'{fig_dir}/{basin}_dhdt_avg{rolling}_map.png'\n",
    "    if save: \n",
    "        print('Saving figure...', end='', flush=True)\n",
    "        fig.savefig(figname, dpi=dpi, bbox_inches='tight', transparent=transparent)\n",
    "        cfig.savefig(cbarname, dpi=dpi/2, bbox_inches='tight', transparent=transparent)     \n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def make_ensemble_figs(save=False):\n",
    "    imagery_aspect = 1.4\n",
    "    major_font_size = 16\n",
    "    minor_font_size = 16\n",
    "    line_w = 0.5\n",
    "    figsize=[8, 2.6]\n",
    "    legendsize=[5, 1.5]\n",
    "    \n",
    "    fig_list = []\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    boxprops = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none', pad=0.2)\n",
    "    \n",
    "    hdls=[]\n",
    "    ano_cycle_fl = ds_fl.h_ano.median(dim=['x', 'track', 'pt'])\n",
    "    ano_cycle_gr = ds_gr.h_ano.median(dim=['x', 'track', 'pt'])\n",
    "    ano_cycle_pp = ds_pp.h_ano.median(dim=['x', 'track', 'pt'])\n",
    "    ax.axhline(y=0.0, linestyle='--', color='black')\n",
    "    hdl, = ax.plot(ano_cycle_fl.cycle_number, (ano_cycle_fl - ano_cycle_fl.isel(cycle_number=0)), color='red', label='floating ice')\n",
    "    hdls.append(hdl)\n",
    "    hdl, = ax.plot(ano_cycle_pp.cycle_number, (ano_cycle_pp - ano_cycle_pp.isel(cycle_number=0)), '--', color='red', label='pinning points')\n",
    "    hdls.append(hdl)\n",
    "    hdl, = ax.plot(ano_cycle_gr.cycle_number, (ano_cycle_gr - ano_cycle_gr.isel(cycle_number=0)), '-', color='lightblue', label='upstream grounded ice')\n",
    "    hdls.append(hdl)\n",
    "    # starting at 2 is 2019\n",
    "    ax.set_xlim([2, 24])\n",
    "    ax.set_ylim([-0.9, 1.1])\n",
    "    ax.set_xticks(ticks=np.arange(2, 24), minor=True)\n",
    "    ax.set_xticks(ticks=np.arange(2, 24, 4), labels=[f'{int(c)}' for c in np.arange(2019, 2025)])\n",
    "    ax.set_yticks(ticks=[-0.5, 0.5])\n",
    "    ax.tick_params(axis='x', which='major', length=5, width=2)\n",
    "    ax.tick_params(axis='x', which='minor', length=3, width=1)\n",
    "    ax.tick_params(which='both', direction='in', zorder=1, labelsize=minor_font_size)\n",
    "    ax.set_ylabel(' height (m)', fontsize=minor_font_size, labelpad=-8)\n",
    "    #ax.text(0.5, 0.96, f'{basin} Median Height Anomaly', transform=ax.transAxes, ha='center', va='top', \n",
    "    #fontsize=major_font_size, bbox=boxprops)\n",
    "    ax.text(0.5, 0.96, f'Median Height Anomaly', transform=ax.transAxes, ha='center', va='top', \n",
    "    fontsize=major_font_size, bbox=boxprops)\n",
    "    #ax.legend(loc='lower left', fontsize=minor_font_size)\n",
    "    fig_list.append(fig)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig, ax = plt.subplots(figsize=legendsize)\n",
    "    ax.axis('off')\n",
    "    legend = ax.legend(handles, labels, loc='center', fontsize=minor_font_size)\n",
    "    ax.add_artist(legend)\n",
    "    ax.set_facecolor('white')\n",
    "    fig_list.append(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    #anomaly (data) count\n",
    "    ano_count_fl = ds_fl.h_ano.count(dim=['x', 'track', 'pt'])\n",
    "    ano_count_gr = ds_gr.h_ano.count(dim=['x', 'track', 'pt'])\n",
    "    ano_count_pp = ds_pp.h_ano.count(dim=['x', 'track', 'pt'])\n",
    "    \n",
    "    hdls = []\n",
    "    ax.axhline(y=1.0, linestyle='--', color='black')\n",
    "    hdl = ax.plot(ano_cycle_fl.cycle_number, ano_count_fl/ano_count_fl.median(), color='red', label=f'mean = {int(ano_count_fl.mean().data)} values')\n",
    "    hdls.append(hdl)\n",
    "    hdl = ax.plot(ano_cycle_pp.cycle_number, ano_count_pp/ano_count_pp.median(), '--', color='red', label=f'mean = {int(ano_count_pp.mean().data)} values')\n",
    "    hdls.append(hdl)\n",
    "    hdl = ax.plot(ano_cycle_gr.cycle_number, ano_count_gr/ano_count_gr.median(), '-', color='lightblue', label=f'mean = {int(ano_count_gr.mean().data)} values')\n",
    "    hdls.append(hdl)\n",
    "    ax.set_xlim([2, 24])\n",
    "    ax.set_ylim([-0.0, 1.75])\n",
    "    ax.set_yticks([0.5, 1.0, 1.5])\n",
    "    ax.set_ylabel('count fraction', fontsize=minor_font_size)\n",
    "    ax.set_xticks(ticks=np.arange(2, 24), minor=True)\n",
    "    ax.set_xticks(ticks=np.arange(2, 24, 4), labels=[f'{int(c)}' for c in np.arange(2019, 2025)])\n",
    "    ax.tick_params(axis='x', which='major', length=5, width=2, labelsize=minor_font_size)\n",
    "    ax.tick_params(axis='x', which='minor', length=3, width=1)\n",
    "    ax.tick_params(which='both', direction='in', zorder=1, labelsize=minor_font_size)\n",
    "    ax.text(0.5, 0.96, f'{basin} Relative Elevation Count', transform=ax.transAxes, ha='center', va='top', fontsize=major_font_size, bbox=boxprops)\n",
    "    #ax.legend(loc='lower left', fontsize=minor_font_size)\n",
    "    fig_list.append(fig)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig, ax = plt.subplots(figsize=legendsize)\n",
    "    ax.axis('off')\n",
    "    legend = ax.legend(handles, labels, loc='center', fontsize=minor_font_size)\n",
    "    ax.add_artist(legend)\n",
    "    ax.set_facecolor('white')\n",
    "    fig_list.append(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.gca().set_facecolor('white')\n",
    "    bin_edges = np.arange(-10, 260, 10)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:]) #for step fn\n",
    "    h_fl = ds_fl.h_abs.median(dim='cycle_number')\n",
    "    h_pp = ds_pp.h_abs.median(dim='cycle_number')\n",
    "    h_gr = ds_gr.h_abs.median(dim='cycle_number')\n",
    "    ax.hist(np.ndarray.flatten(h_gr.data), bin_edges, \n",
    "    color='lightblue', edgecolor='black', alpha=1,\n",
    "    label=f'{basin} floating ice', density=True);\n",
    "    ax.hist(np.ndarray.flatten(h_fl.data), bin_edges, \n",
    "    color='darkred', edgecolor='black', alpha=1,\n",
    "    label=f'{basin} floating ice', density=True);\n",
    "    ax.hist(np.ndarray.flatten(h_pp.data), bin_edges, \n",
    "    color='white', edgecolor='darkred', alpha=0.6, hatch=None,\n",
    "    label=f'{basin} pinning points', density=True);\n",
    "    counts, _ = np.histogram(np.ndarray.flatten(h_pp.data), bins=bin_edges, density=True)\n",
    "    ax.step(bin_centers, counts, where='mid', linestyle='-', color='white')\n",
    "    ax.step(bin_centers, counts, where='mid', linestyle='--', color='darkred')\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    ax.set_ylabel('count density', fontsize=minor_font_size, labelpad=-9)\n",
    "    ax.tick_params(axis='y', colors='none')\n",
    "    ax.set_ylim([0, 0.035])\n",
    "    ax.set_xlabel('height (m)', fontsize=minor_font_size)\n",
    "    ax.tick_params(which='both', direction='in', zorder=1, labelsize=minor_font_size)\n",
    "    #ax.text(0.5, 0.96, f'{basin} Median elevation distribution', transform=ax.transAxes, ha='center', va='top', fontsize=major_font_size, bbox=boxprops)\n",
    "    ax.text(0.5, 0.96, f'Median elevation distribution', transform=ax.transAxes, ha='center', va='top', fontsize=major_font_size, bbox=boxprops)\n",
    "    ax.set_xlim([-10, 250])\n",
    "    fig_list.append(fig)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig, ax = plt.subplots(figsize=legendsize)\n",
    "    ax.axis('off')\n",
    "    legend = ax.legend(handles, labels, loc='center', fontsize=minor_font_size)\n",
    "    ax.add_artist(legend)\n",
    "    ax.set_facecolor('white')\n",
    "    fig_list.append(fig)\n",
    "    \n",
    "    if save:\n",
    "        fig_list[0].savefig(f'{fig_dir}/{basin}_median_anom.png', \n",
    "            dpi=300, bbox_inches='tight', transparent=False)\n",
    "        fig_list[1].savefig(f'{fig_dir}/{basin}_median_anom_legend.png',\n",
    "            dpi=300, bbox_inches='tight', transparent=False)\n",
    "        fig_list[2].savefig(f'{fig_dir}/{basin}_median_count.png',\n",
    "            dpi=300, bbox_inches='tight', transparent=False)\n",
    "        fig_list[3].savefig(f'{fig_dir}/{basin}_median_count_legend.png',\n",
    "            dpi=300, bbox_inches='tight', transparent=False)\n",
    "        fig_list[4].savefig(f'{fig_dir}/{basin}_h_abs_hist.png',\n",
    "            dpi=300, bbox_inches='tight', transparent=False)\n",
    "\n",
    "    for fig in fig_list: plt.close(fig)\n",
    "\t\n",
    "    return fig_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating map variable..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/fresh/lib/python3.12/site-packages/xarray/core/nputils.py:248: RankWarning: Polyfit may be poorly conditioned\n",
      "  warn_on_deficient_rank(rank, x.shape[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE in 274.05 s\n"
     ]
    }
   ],
   "source": [
    "def calculate_map_variable(map_variable):\n",
    "    if map_variable=='h_ano_annual': \n",
    "        for y in np.arange(2018, 2025, 1): \n",
    "            cycle_arr = np.arange((y-2018)*4, (y-2018)*4+4, 1)\n",
    "            h_plot = ds_all.h_ano()\n",
    "    elif map_variable=='h_ano_lin':\n",
    "        h_plot = ds_all.h_ano.polyfit('cycle_number', deg=1, skipna=True)\n",
    "        h_plot = h_plot.sel(degree=1).polyfit_coefficients\n",
    "    elif map_variable=='determination_coeff':\n",
    "        h_ano_lin = ds_all.h_ano.polyfit('cycle_number', deg=1, skipna=True)\n",
    "        m, b = h_ano_lin.sel(degree=1).polyfit_coefficients, h_ano_lin.sel(degree=0).polyfit_coefficients\n",
    "        h_hat = ds_all.cycle_number*m+b\n",
    "        r2 = 1-((ds_all.h_ano() - h_hat)**2).sum(dim='cycle_number')/((ds_all.h_ano() - ds_all.h_ano().mean(dim='cycle_number'))**2).sum(dim='cycle_number')\n",
    "        h_plot = r2\n",
    "    return h_plot\n",
    "\n",
    "print('Calculating map variable...', end='', flush=True)\n",
    "tic = time.time()\n",
    "h_plot = calculate_map_variable('h_ano_lin')\n",
    "toc = time.time()\n",
    "print(f'DONE in {int((toc-tic)/60)}m {(toc-tic)%60:.0f} s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating map...succesfful\n",
      "/Users/ccroberts/Applications/buttressing/figs/Cp-D_dhdt_map.png\n",
      "Saving figure...DONE\n",
      "Generating ensemble figs...DONE\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x25 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## Run the plot functions ##\n",
    "\n",
    "print('Generating map...', end='', flush=True)\n",
    "fig = make_map(h_plot, dpi=100, vlims=[-1, 1], rolling=50, save=True, transparent=True)\n",
    "print('DONE')\n",
    "\n",
    "print('Generating ensemble figs...', end='', flush=True)\n",
    "#fig_list = make_ensemble_figs(save=True)\n",
    "print('DONE')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
